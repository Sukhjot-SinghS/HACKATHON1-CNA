import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline

# --- 1. Load and Clean Training Data ---
train_df = pd.read_csv('hacktrain.csv')
ndvi_cols = [col for col in train_df.columns if col.endswith('N')]
train_df[ndvi_cols] = train_df[ndvi_cols] / 10000

# NDVI cleaning: interpolate and median neighbor fill for abnormal/missing values
ndviarr = train_df[ndvi_cols].values
abno = (train_df[ndvi_cols].isnull()) | (train_df[ndvi_cols].diff(axis=1).abs() > 0.5)
for i in range(ndviarr.shape[0]):
    for j in range(ndviarr.shape[1]):
        neighbors = []
        if abno.iloc[i, j]:
            if j > 0 and not abno.iloc[i, j-1]:
                neighbors.append(ndviarr[i, j-1])
            if j < ndviarr.shape[1] - 1 and not abno.iloc[i, j+1]:
                neighbors.append(ndviarr[i, j+1])
            if len(neighbors) == 2:
                ndviarr[i, j] = np.median(neighbors)
train_df[ndvi_cols] = ndviarr
train_df[ndvi_cols] = train_df[ndvi_cols].interpolate(axis=1, limit_direction='both')

# --- 2. Feature Engineering: Seasonal, Trend, and Polynomial Features ---
def get_year_month(col):
    date_str = col[:8]
    year = int(date_str[:4])
    month = int(date_str[4:6])
    return year, month

seasons = {'Winter': [12, 1, 2], 'Spring': [3, 4, 5], 'Summer': [6, 7, 8], 'Fall': [9, 10, 11]}
season_map = {}
for col in ndvi_cols:
    year, month = get_year_month(col)
    for season, months in seasons.items():
        if month in months:
            season_year = year + 1 if (season == 'Winter' and month == 12) else year
            season_map.setdefault((season_year, season), []).append(col)
            break

for (season_year, season), cols in season_map.items():
    if cols:
        train_df[f'{season_year}_{season}_mean'] = train_df[cols].mean(axis=1)
        train_df[f'{season_year}_{season}_std'] = train_df[cols].std(axis=1)
        train_df[f'{season_year}_{season}_max'] = train_df[cols].max(axis=1)
        train_df[f'{season_year}_{season}_min'] = train_df[cols].min(axis=1)
        train_df[f'{season_year}_{season}_amplitude'] = train_df[f'{season_year}_{season}_max'] - train_df[f'{season_year}_{season}_min']

sorted_ndvi_cols = sorted(ndvi_cols, key=lambda x: x[:8])
ndvi_values = train_df[sorted_ndvi_cols].values
slopes = []
for row in ndvi_values:
    x = np.arange(len(row))
    if np.any(np.isnan(row)):
        row = pd.Series(row).interpolate().fillna(method='bfill').fillna(method='ffill').values
    slope = np.polyfit(x, row, 1)[0]
    slopes.append(slope)
train_df['ndvi_slope'] = slopes
train_df['ndvi_peak'] = train_df[sorted_ndvi_cols].max(axis=1)
train_df['ndvi_trough'] = train_df[sorted_ndvi_cols].min(axis=1)
train_df['ndvi_amplitude'] = train_df['ndvi_peak'] - train_df['ndvi_trough']

# --- Polynomial Features (degree 2) ---
feature_cols = [col for col in train_df.columns if any(s in col for s in ['mean', 'std', 'max', 'min', 'amplitude', 'slope'])]
X_base = train_df[feature_cols]
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X_base)

# --- 3. Encode Labels ---
label_encoder = LabelEncoder()
train_df['class'] = label_encoder.fit_transform(train_df['class'].astype(str))
y = train_df['class']

# --- 4. Model Training with Scaling and GridSearchCV ---
pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('clf', LogisticRegression(
        solver='lbfgs',
        class_weight='balanced'
    ))
])
param_grid = {
    'clf__C': [1, 10, 50, 100],
    'clf__max_iter': [300, 600, 900]
}
grid = GridSearchCV(pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)
grid.fit(X_poly, y)

# --- 5. Prepare Test Data (Same Feature Engineering) ---
test_df = pd.read_csv('hacktest.csv')
ID = test_df['ID']
test_df[ndvi_cols] = test_df[ndvi_cols] / 10000
ndviarr_test = test_df[ndvi_cols].values
abno_test = (test_df[ndvi_cols].isnull()) | (test_df[ndvi_cols].diff(axis=1).abs() > 0.5)
for i in range(ndviarr_test.shape[0]):
    for j in range(ndviarr_test.shape[1]):
        neighbors = []
        if abno_test.iloc[i, j]:
            if j > 0 and not abno_test.iloc[i, j-1]:
                neighbors.append(ndviarr_test[i, j-1])
            if j < ndviarr_test.shape[1] - 1 and not abno_test.iloc[i, j+1]:
                neighbors.append(ndviarr_test[i, j+1])
            if len(neighbors) == 2:
                ndviarr_test[i, j] = np.median(neighbors)
test_df[ndvi_cols] = ndviarr_test
test_df[ndvi_cols] = test_df[ndvi_cols].interpolate(axis=1, limit_direction='both')

for (season_year, season), cols in season_map.items():
    if cols:
        test_df[f'{season_year}_{season}_mean'] = test_df[cols].mean(axis=1)
        test_df[f'{season_year}_{season}_std'] = test_df[cols].std(axis=1)
        test_df[f'{season_year}_{season}_max'] = test_df[cols].max(axis=1)
        test_df[f'{season_year}_{season}_min'] = test_df[cols].min(axis=1)
        test_df[f'{season_year}_{season}_amplitude'] = test_df[f'{season_year}_{season}_max'] - test_df[f'{season_year}_{season}_min']

ndvi_values_test = test_df[sorted_ndvi_cols].values
slopes_test = []
for row in ndvi_values_test:
    x = np.arange(len(row))
    if np.any(np.isnan(row)):
        row = pd.Series(row).interpolate().fillna(method='bfill').fillna(method='ffill').values
    slope = np.polyfit(x, row, 1)[0]
    slopes_test.append(slope)
test_df['ndvi_slope'] = slopes_test
test_df['ndvi_peak'] = test_df[sorted_ndvi_cols].max(axis=1)
test_df['ndvi_trough'] = test_df[sorted_ndvi_cols].min(axis=1)
test_df['ndvi_amplitude'] = test_df['ndvi_peak'] - test_df['ndvi_trough']

X_test_base = test_df[feature_cols]
X_test_poly = poly.transform(X_test_base)

# --- 6. Predict and Prepare Submission ---
predictions = grid.predict(X_test_poly)
decoded_predictions = label_encoder.inverse_transform(predictions)
results_df = pd.DataFrame({'ID': ID, 'class': decoded_predictions})
results_df.to_csv('submission_results.csv', index=False)
print(results_df.head())
